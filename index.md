---
title: LLM Eval Benchmark Lab
---

# ðŸ§ª LLM Eval Benchmark Lab

A modular evaluation system for GenAI teams who need structured model comparison across reasoning, safety, and latency tradeoffs.

> Built by Eva Paunova â€“ [GitHub](https://github.com/epaunova) | [LinkedIn](https://www.linkedin.com/in/eva-hristova-paunova-a194b3210/)

## Features
âœ… YAML-configurable eval runs  
âœ… GPT-based or rule-based scoring  
âœ… Reasoning and safety modules  
âœ… Notebooks for analysis

ðŸ‘‰ [View project on GitHub](https://github.com/epaunova/llm-eval-benchmark-lab)
