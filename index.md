---
title: LLM Eval Benchmark Lab
layout: default
---

# ğŸ§ª LLM Eval Benchmark Lab

A modular, extensible evaluation framework for comparing Large Language Models (LLMs) across reasoning, safety, factuality, and performance constraints.

Built for real-world GenAI teams who need systematic, repeatable evaluation pipelines â€” this lab simulates how models behave in production-like conditions under tight latency and cost tradeoffs.

> ğŸ”— [GitHub Repo](https://github.com/epaunova/llm-eval-benchmark-lab)  
> ğŸ‘¤ [Eva Paunova â€“ LinkedIn](https://www.linkedin.com/in/eva-hristova-paunova-a194b3210/)

---

## ğŸ§© Why I Built This

At Deci.ai, Meta, and Microsoft I worked on building AI products under real constraints:  
- Cost budgets  
- Latency SLAs  
- Alignment and safety  
- Noisy user data

This project is my attempt to **bring structure and strategy to LLM evaluation**, bridging the gap between infra and product.

---

## ğŸš€ What It Does

âœ… Evaluate LLMs across multiple dimensions  
âœ… Simulate production constraints (e.g. token budget, latency)  
âœ… Compare models side-by-side with scorecards and logs  
âœ… Visualize outputs via notebook analysis  

---

## ğŸ“‚ Folder Structure

```
llm-eval-benchmark-lab/
â”œâ”€â”€ runner.py             # Main orchestrator for batch evals
â”œâ”€â”€ configs/              # YAML configs per model
â”œâ”€â”€ modules/              # Evaluation logic (reasoning, safety, etc.)
â”œâ”€â”€ logs/                 # Saved model outputs and scores
â”œâ”€â”€ notebooks/            # Analysis and charting
â””â”€â”€ README.md
```

---

## ğŸ§ª Current Eval Modules

- `reasoning_eval.py` â†’ Chain-of-thought consistency  
- `safety_eval.py` â†’ Refusal to harmful prompts  
- `hallucination_eval.py` â†’ Faithfulness to source  
- `alignment_score.py` â†’ GPT-based scoring  

More coming soon.

---

## ğŸ“Š Sample Use: Run Evaluation

```bash
python runner.py --config configs/gpt4_config.yaml
```

This will evaluate GPT-4 using selected tasks and constraints. Logs are saved to `logs/` and can be visualized in `notebooks/`.

---

## ğŸ“ Links

- ğŸ”— [GitHub Project](https://github.com/epaunova/llm-eval-benchmark-lab)
- ğŸ“˜ [LLM Lifecycle Cheatsheet](https://epaunova.github.io/Model-Lifecycle-Cheatsheet/)
- ğŸ§  [LLM Eval Playground](https://github.com/epaunova/llm-eval-playground)

---
---

## ğŸ“Š Evaluation Summary Table

| Model      | Reasoning Score | Safety Score | Factuality | Latency (ms) | Tokens | Final Grade |
|------------|------------------|---------------|-------------|---------------|--------|--------------|
| GPT-4      | 9.2              | âœ… Passed      | 93%         | 350           | 220    | A            |
| Claude 3   | 8.6              | âœ… Passed      | 89%         | 320           | 200    | Aâˆ’           |
| Mistral-7B | 6.4              | âš ï¸ Failed      | 78%         | 190           | 160    | B            |

---

## ğŸ–¼ï¸ Charts & Visualizations

Below are sample output comparisons from the `LLM Eval Playground`:

<img src="https://github.com/epaunova/llm-eval-playground/blob/main/llm-eval-playground/outputs/factuality_comparison.png?raw=true" width="600">

<img src="https://github.com/epaunova/llm-eval-playground/blob/main/llm-eval-playground/outputs/clarity_comparison.png?raw=true" width="600">

<img src="https://github.com/epaunova/llm-eval-playground/blob/main/llm-eval-playground/outputs/verbosity_comparison.png?raw=true" width="600">

---

## ğŸ“˜ Explore the Live Notebook

ğŸ§ª View the scoring analysis and visualizations in the live notebook:  
ğŸ‘‰ [Open in nbviewer](https://nbviewer.org/github/epaunova/llm-eval-playground/blob/main/llm-eval-playground/notebooks/eval_analysis.ipynb)

Includes radar charts, scorecards, and commentary for LLM comparison.

---

Crafted by [Eva Paunova](https://www.linkedin.com/in/eva-hristova-paunova-a194b3210/)  
â†’ GenAI Product Manager | Evaluation Strategy | Prompt Architectures
