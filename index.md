---
title: LLM Eval Benchmark Lab
---

# ðŸ§ª LLM Eval Benchmark Lab

A modular evaluation system for GenAI teams who need structured model comparison across reasoning, safety, and latency tradeoffs.

> Built by Eva Paunova â€“ [GitHub](https://github.com/epaunova) | [LinkedIn](https://www.linkedin.com/in/eva-hristova-paunova-a194b3210/)

---

## ðŸ§© Why I Built This

At Deci.ai, Meta, and Microsoft I worked on building AI products under real constraints:  
- Cost budgets  
- Latency SLAs  
- Alignment and safety  
- Noisy user data

This project brings structure and strategy to LLM evaluation, bridging the gap between infra and product.

---

## ðŸš€ What It Does

âœ… Evaluate LLMs across multiple dimensions  
âœ… Simulate production constraints (e.g. token budget, latency)  
âœ… Compare models side-by-side with scorecards and logs  
âœ… Visualize outputs via notebook analysis  

---

## ðŸ“‚ Folder Structure

